{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e23231fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e85a12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb62d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome readers.', 'I hope you find it interesting.', 'Please do reply.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "# 텍스트를 문장으로 토큰화\n",
    "text=\"Welcome readers. I hope you find it interesting. Please do reply.\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a050b9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello everyone.',\n",
       " 'Hope all are fine and doing well.',\n",
       " 'Hope yu find the book interesting']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PunkSentenseTokenizer 로드 뒤 토큰화\n",
    "tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "text2=\"Hello everyone. Hope all are fine and doing well. Hope yu find the book interesting\"\n",
    "tokenizer.tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77fdecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deux aggressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collége franco-britanniquedeLevallois- Perret.',\n",
       " 'Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois.',\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire.\",\n",
       " \"L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다양한 언어의 텍스트 토큰화\n",
    "french_tokenizer=nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "french_tokenizer.tokenize(\"Deux aggressions en quelques jours, voilà ce qui a motivé hier matin le débrayage collége franco-britanniquedeLevallois- Perret. Deux agressions en quelques jours, voilà ce qui a motivé hier matin le débrayage Levallois. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, janvier , d'un professeur d'histoire. L'équipe pédagogique de ce collège de 750 élèves avait déjà été choquée par l'agression, mercredi , d'un professeur d'histoire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af7fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PierreVinken', ',', '59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'on', 'Nov.', '29', '.', '>', '>']\n"
     ]
    }
   ],
   "source": [
    "# 문장을 단어로 토큰화 - NLTK 인스턴스 \n",
    "text3=nltk.word_tokenize(\"PierreVinken , 59 years old , will join as a nonexecutive director on Nov. 29 .>>\")\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce246582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a textToday is a pleasant day\n"
     ]
    }
   ],
   "source": [
    "# 단어 입력 확인 & 토큰화 후 길이 확인\n",
    "from nltk import word_tokenize\n",
    "r=input(\"Please write a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c5dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of text is 5 words\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of text is\",len(word_tokenize(r)),\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eadb8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Have',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'day.',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'find',\n",
       " 'the',\n",
       " 'book',\n",
       " 'interesting']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TreebankWordTokenizer를 사용한 토큰화\n",
    "from nltk.tokenize import TreebankWordTokenizer  #펜 트리뱅크 코퍼스\n",
    "tokenizer2 = TreebankWordTokenizer()\n",
    "tokenizer2.tokenize(\"Have a nice day. I hope you find the book interesting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44de7cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "text4=nltk.word_tokenize(\" Don't hesitate to ask questions\")  # 분리된 축약형\n",
    "print(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87ab109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dom', \"'\", 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PunktWordTokenizer - 분리된 문장 부호로 작동, 생성 대신 유지\n",
    "# WordPunctTokenizer를 사용한 토큰화 - 새로운 토큰으로 분할\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer3=WordPunctTokenizer()\n",
    "tokenizer3.tokenize(\" Dom't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c432ea3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don', 't', 'hesitate', 'to', 'ask', 'questions']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식을 사용한 토큰화 : 단어, 스페이스 혹은 공백으로 매칭\n",
    "# RegexpTokenizer를 사용한 토큰화\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer4=RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer4.tokenize(\"Don't hesitate to ask questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89a1446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'t\", 'hesitate', 'to', 'ask', 'questions']\n"
     ]
    }
   ],
   "source": [
    "# regexp_tokenize 함수를 사용한 토큰화\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "sent=\"Don't hesitate to ask questions\"\n",
    "print(regexp_tokenize(sent, pattern='\\w+|\\$[\\d\\.]+|\\S+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07e6c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'She']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대문자로 시작되는 단어 선택\n",
    "sent2=\" She secured 90.65 % in class X . She is a meritorious student\"\n",
    "capt = RegexpTokenizer('[A-Z]\\w+')\n",
    "capt.tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e64b133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' She secured 90.56 % in class X . She is a meritorios student']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RegexpTokenizer의 서브클래스 BlindlineTokenizer\n",
    "sent3=\" She secured 90.56 % in class X . She is a meritorios student\"\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "BlanklineTokenizer().tokenize(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef735af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'secured',\n",
       " '90.56',\n",
       " '%',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X',\n",
       " '.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorios',\n",
       " 'student']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RegexpTokenizer의 서브클래스 WhitespaceTokenizer \n",
    "sent4=\" She secured 90.56 % in class X . She is a meritorios student\"\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "WhitespaceTokenizer().tokenize(sent4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ff35b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She',\n",
       " 'secured',\n",
       " '90.56',\n",
       " '%',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorious',\n",
       " 'student']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split() 메소드를 사용한 토큰화\n",
    "sent5= \"She secured 90.56 % in class X. She is a meritorious student\"\n",
    "sent5.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbce47af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' She secured 90.56 % in class X \\n. She is a meritorious student\\n']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BlanklineTokenizer\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "sent6=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "BlanklineTokenizer().tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37c6481e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' She secured 90.56 % in class X ', '. She is a meritorious student']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LineTokenizer : 텍스트를 라인으로 토큰화 \n",
    "from nltk.tokenize import LineTokenizer\n",
    "LineTokenizer(blanklines='keep').tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c22fcc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'She',\n",
       " 'secured',\n",
       " '90.56',\n",
       " '%',\n",
       " 'in',\n",
       " 'class',\n",
       " 'X',\n",
       " '\\n.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'a',\n",
       " 'meritorious',\n",
       " 'student\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SpaceTokenizer : split()과 유사하게 작동\n",
    "sent7=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "SpaceTokenizer().tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c393058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4),\n",
       " (5, 12),\n",
       " (13, 18),\n",
       " (19, 20),\n",
       " (21, 23),\n",
       " (24, 29),\n",
       " (30, 31),\n",
       " (33, 34),\n",
       " (35, 38),\n",
       " (39, 41),\n",
       " (42, 43),\n",
       " (44, 55),\n",
       " (56, 63)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.tokenize.util 모듈 : 문장에서 토큰의 오프셋인 튜플의 순서 반환\n",
    "sent8=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "list(WhitespaceTokenizer().span_tokenize(sent8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2078673e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3),\n",
       " (1, 7),\n",
       " (1, 5),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 5),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (1, 3),\n",
       " (1, 2),\n",
       " (1, 1),\n",
       " (1, 11),\n",
       " (1, 7)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스팬의 순서가 주어지면 상대 스팬의 순서 반환\n",
    "from nltk.tokenize.util import spans_to_relative\n",
    "sent9=\" She secured 90.56 % in class X \\n. She is a meritorious student\\n\"\n",
    "list(spans_to_relative(WhitespaceTokenizer().span_tokenize(sent9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37060b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening', '.'], ['Guests', ',', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 정규화\n",
    "# 토큰화된 텍스트\n",
    "text5=[\" It is a pleasant evening.\", \"Guests, who came from US arrived at the venue\", \"Food was tasty.\"]\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text5]\n",
    "print(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817ced34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'is', 'a', 'pleasant', 'evening'], ['Guests', 'who', 'came', 'from', 'US', 'arrived', 'at', 'the', 'venue'], ['Food', 'was', 'tasty']]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 텍스트에서 문장 부호 제거\n",
    "import re\n",
    "import string\n",
    "text6=[\" It is a pleasant evening.\",\"Guests, who came from US arrived at the venue\",\"Food was tasty.\"]\n",
    "tokenized_docs=[word_tokenize(doc) for doc in text6]\n",
    "x=re.compile('[%s]' % re.escape(string.punctuation))\n",
    "tokenized_docs_no_punctuation = []\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = x.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b84a712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harswork is key to success\n",
      "HARSWORK IS KEY TO SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# 소문자와 대문자로 변환\n",
    "text7='HARsWork IS KEy to SUCCESS'\n",
    "print(text7.lower())\n",
    "print(text7.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b676015f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Don't\", 'hesitate', 'ask', 'questions']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 처리\n",
    "# nltk.corpus.reader.WordListCorpusReader의 인스턴스는 stopwords 코퍼스\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words('english'))\n",
    "words=[\"Don't\", 'hesitate', 'to', 'ask', 'questions']\n",
    "[word for word in words if word not in stops]\n",
    "\n",
    "# 코퍼스에 인자가 filled인 words() 함수가 있는 경우 - 영어로, 영어 파일에 존재하는 모든 불용어\n",
    "# words() 함수에 인자가 없는 경우 - 모든 언어의 모든 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad1390cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'basque',\n",
       " 'bengali',\n",
       " 'catalan',\n",
       " 'chinese',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hebrew',\n",
       " 'hinglish',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 제거 수행 가능한 언어 혹은 nltk에 불용어 파일이 존재하는 언어 확인 - words() 함수 인자\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd0ef0fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어의 불용어 계산\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "227f3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 text에서 불용어 제거\n",
    "# stopwords : NLTK에서 제공하는 불용어 목록, para : 불용어를 제외한 단어 리스트\n",
    "def para_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    para = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(para) / len(text)  # 불용어를 제외한 단어의 수 / 전체 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fdc00e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735240435097661"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 라이브러리의 Reuters 뉴스 데이터셋에서 불용어를 제외한 단어 비율\n",
    "para_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff385951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5240253497361037"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 라이브러리의 미국 대통령 취임 연설에서 불용어를 제외한 단어 비율\n",
    "para_fraction(nltk.corpus.inaugural.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83691a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰의 대체 및 수정\n",
    "# 정규 표현식을 사용한 단어 대체\n",
    "replacement_patterns = [  # 첫 번째 용어 - 일치, 두 번째 용어 - 대응 패턴으로 정의해서 패턴 쌍 컴파일\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'can\\'t', 'cannot'),\n",
    "    (r'i\\'m', 'i am'),\n",
    "    (r'ain\\'t', 'is not'),\n",
    "    (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', 'g<1> would')\n",
    "]\n",
    "# RegexpReplacer 클래스 - 패턴 쌍을 컴파일하는 작업 수행, replace 메소드 제공\n",
    "class RegexpReplacer(object):\n",
    "    def __init__(self, patterns=replacement_patterns):  \n",
    "        self.patterns = [(re.compile(regex), repl) for (regex, repl)\n",
    "in patterns]\n",
    "    def replace(self, text):  # replace() : 패턴을 다른 패턴으로 대체\n",
    "        s = text\n",
    "        for (pattern, repl) in self.patterns:\n",
    "            (s, count) = re.subn(pattern, repl, s)\n",
    "        return s  # replacers.py로 같은 폴더 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c26d071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트에 지프의 법칙 적용 - 문서의 단어 빈도와 순위 빈도 간의 비례 관계\n",
    "# 양대수 그래프\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('TkAgg')\n",
    "fd = FreqDist()\n",
    "for text in gutenberg.fileids():\n",
    "    for word in gutenberg.words(text):\n",
    "        fd[word] += 1\n",
    "ranks = []\n",
    "freqs = []\n",
    "for rank, word in enumerate(fd):\n",
    "    ranks.append(rank+1)\n",
    "    freqs.append(fd[word])\n",
    "    \n",
    "plt.loglog(ranks, freqs)\n",
    "plt.xlabel('frequency(f)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('rank(r)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0e37d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "1.0\n",
      "0.6666666666666666\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# 유사 척도 (NLP) - 성능 테스트 위해 표준 점수 사용 \n",
    "# 훈련 파일에서 얻은 표준 점수를 사용한 개체명 인식기의 출력 분석\n",
    "from __future__ import print_function \n",
    "from nltk.metrics import *\n",
    "training='PERSON OTHER PERSON OTHER OTHER ORGANIZATION'.split()\n",
    "testing='PERSON OTHER OTHER OTHER OTHER OTHER'.split()\n",
    "print(accuracy(training,testing))\n",
    "trainset=set(training)\n",
    "testset=set(testing)\n",
    "print(precision(trainset,testset))\n",
    "print(recall(trainset,testset))\n",
    "print(f_measure(trainset,testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022a6009-d480-47cf-bc4f-5bf0866e56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편집 거리 알고리즘을 사용한 유사 척도\n",
    "# 두 문자열 간의 편집 거리 (리벤슈타인 편집 거리)\n",
    "def _edit_dist_init(len1, len2):\n",
    "    lev = []\n",
    "    for i in range(len1):\n",
    "        lev.append([0] * len2)  # 2차원 배열을 0으로 초기화\n",
    "    for i in range(len1):\n",
    "        lev[i][0] = i           # 열 0: 0,1,2,3,4,... \n",
    "    for j in range(len2):\n",
    "        lev[0][j] = j           # 행 0: 0,1,2,3,4,...\n",
    "    return lev\n",
    "\n",
    "def _edit_dist_step(lev,i,j,s1,s2,transpositions=False):\n",
    "    c1 = s1[i-1]\n",
    "    c2 = s2[j-1]\n",
    "    \n",
    "    # s1에서 문자 스킵\n",
    "    a = lev[i-1][j] + 1\n",
    "    # s2에서 문자 스킵\n",
    "    b = lev[i][j-1] + 1\n",
    "    # 대체\n",
    "    c = lev[i-1][j-1]+(c1!=c2)\n",
    "    # 이항\n",
    "    d = c+1  # 기본적으로 선택되지 않음\n",
    "    if transpositions and i>1 and j>1:\n",
    "        if s1[i-2]==c2 and s2[j-2]==c1:\n",
    "            d = lev[i-2][j-2]+1\n",
    "    # 최소값 선택\n",
    "    lev[i][j] = min(a,b,c,d)\n",
    "    \n",
    "def edit_distance(s1, s2, transpositions=False):\n",
    "    # 2차원 배열 설정\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "    lev = _edit_dist_init(len1 + 1, len2 + 1)\n",
    "        \n",
    "    # 배열을 반복\n",
    "    for i in range(len1):\n",
    "        for j in range(len2):\n",
    "            _edit_dist_step(lev, i + 1, j + 1, s1, s2, transpositions=transpositions)\n",
    "            return lev[len1][len2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "618a590f-f1c3-47f2-9b5b-9318ce2f3bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# nltk.metrics 패키지를 사용한 NLTK에서 계산된 편집 거리\n",
    "from nltk.metrics import *\n",
    "print(edit_distance(\"relate\",\"relation\"))  # 1번의 대체와 2번의 삽입\n",
    "print(edit_distance(\"suggestion\",\"calculation\"))  # 6번의 대체와 1번의 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640287d5-fc21-4194-9317-f4aa0203450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자카드 계수를 사용한 유사 척도\n",
    "def jacc_similarity(query, document):\n",
    "    first=set(query).intersection(set(document))\n",
    "    second=set(query).union(set(document))\n",
    "    return len(first)/len(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a654226-325a-4b4a-b88e-8b1f8a9fcb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "X=set([10,20,30,40])\n",
    "Y=set([20,30,60])\n",
    "print(jaccard_distance(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84cd240-5545-4fe1-b4ea-a473bd685964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스미스 워터맨 거리를 사용한 유사 척도\n",
    "# 편집 거리와 유사, NLTK에서 스미스 워터맨을 사용한 문자열 유사도를 수행하는 nltk.metric 패키지에 포함 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21d38e22-ae6c-48bf-bc36-b65b5e2aee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 그 외 문자열 유사도 메트릭\n",
    "# 이진 거리 - 동일할 경우 0, 그렇지 않으면 1 반환\n",
    "def binary_distance(label1, label2):\n",
    "    return 0.0 if label1 == label2 else 1.0\n",
    "X = set([10,20,30,40])\n",
    "Y = set([30,50,70])\n",
    "binary_distance(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c8d7018-399b-484f-849d-268c4f674fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.945\n"
     ]
    }
   ],
   "source": [
    "# 다수의 라벨이 존재하면 매시 거리는 부분 일치에 기초\n",
    "def masi_distance(label1, label2):\n",
    "    len_intersection = len(label1.intersection(label2))\n",
    "    len_union = len(label1.union(label2))\n",
    "    len_label1 = len(label1)\n",
    "    len_label2 = len(label2)\n",
    "    if len_label1 == len_label2 and len_label1 == len_intersection:\n",
    "        m = 1\n",
    "    elif len_intersection == min(len_label1, len_label2):\n",
    "        m = 0.67\n",
    "    elif len_intersection > 0:\n",
    "        m = 0.33\n",
    "    else:\n",
    "        m = 0\n",
    "        \n",
    "    return 1 - (len_intersection / float(len_union)) * m\n",
    "\n",
    "X = set([10,20,30,40])\n",
    "Y = set([30,50,70])\n",
    "print(masi_distance(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4974a96-7775-4b9e-b978-3ded7f023447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpfirst",
   "language": "python",
   "name": "nlpfirst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
